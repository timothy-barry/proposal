\documentclass[12pt]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx} 
\usepackage{float}
\usepackage[caption = false]{subfig}
\usepackage{/Users/timbarry/Documents/optionFiles/mymacros}

\begin{document}
	
% Basic intuition: the type of value (i.e., $p$-value or $e$-value) that we return from a hypothesis test should match the error criterion that we are using. If we are testing a single hypothesis, the natural type-I error target is the probability of incorrectly rejecting the hypothesis under the null. If we are testing a handful of hypotheses, the natural target is the family-wise error rate. Finally, if we are testing a large number of hypotheses, the natural target is false discovery rate. In the first two cases, $p$-values are the most natural objects for controlling type I error; in the final case, by contrast, $e$-values are most natural.

Consider a permutation test (the ideas here apply broadly to all sampling-based tests of independence or conditional independence, including marginal permutation test, conditional permutation test, marginal randomization test, and conditional randomization test). Let $B$ be the number of resamples. Let $T_{1}, \dots, T_B$ be the ordered resampled test statistics. Let $T^*$ be the test statistic computed on the raw data. Let $I_i$ be the indicator that $T^*$ is less than $T_i$, i.e. $$I_i = \mathbb{I}\left( T^* \leq T_i \right).$$ We have the basic fact under the null hypothesis that
$$\E(I_i) = \P(T^* \leq T_i) = i/B.$$ We want to combine $I_1, \dots, I_B$ in such a way so as to produce an e-value $E$ (i.e., a non-negative variable $E$ such that $\E(E) \leq 1$). To test the hypothesis, we check if $E > \beta$ for some threshold $\beta$.

We call $f: \R^B \to \R^{\geq 0}$ a ``combining function'' if $f$ combines $I_1, \dots, I_B$ in such a way so as to produce a valid E-value, i.e.,
$$f(I_1, \dots, I_B) \geq 0; \quad \E\left[f(I_1, \dots, I_B)\right] \leq 1.$$

Let $\mathcal{F}$ be the class of combining functions. An interesting subset of $\mathcal{F}$ is the set of linear combining functions. A function $g \in \mathcal{F}$ is a linear combining function if
$$ g(I_1, \dots, I_B) = c + \sum_{i=1}^B a_i I_i $$ for given scalars $a_1, \dots, a_B,$ and $c.$ The linearity of expectation implies that
$$ \E\left[ g(I_1, \dots, I_B) \right] = c + \sum_{i=1}^n a_i \E(I_i) = c + \frac{1}{B} \sum_{i=1}^B (a_i)(i).$$ Therefore, under the stricter requirement that $\E(E) = 1$, we have the constraints
\begin{itemize}
\item[i] $\sum_{i=1}^B (a_i) i = B(1-c)$
\item[ii] $c + \sum_{i=1}^B a_i I_i \geq 0.$
\end{itemize}
We give a couple examples of linear combining functions. 
\\ \\
\noindent
\textbf{Example 1}. Let $c = 0$ and $a_i = 1/i$ for all $i$. Then 
$$ \sum_{i=1}^B (1/i) i = B(1 - 0),$$ confirming constraint (i). Next, $$ 0 + \sum_{i=1}^B \frac{I_i}{i} \geq 0,$$ confirming constraint (ii). Therefore,
$$ E = \sum_{i=1}^B \frac{I_i}{i}$$ is a valid $e$-value. This is a left-tailed test: if $T^*$ is small, then $E$ is big, leading us to reject the null.
\\ \\
\noindent
\textbf{Example 2}. The standard permutation test is a special case of the proposed framework. Let $c = 3/2 + 1/(2B)$ and $a_i = -1/B$ for all $a_i$. Verifying condition (i),
\begin{multline*}
\sum_{i=1}^B (a_i)i = - \frac{1}{B} \sum_{i=1}^B i =  -\frac{1}{B} \left( \frac{B^2 + B}{2} \right) = -(1/2)B - 1/2 \\ = B(1 - (3/2) - 1/(2B)) = B(1-c).
\end{multline*}
Next, verifying condition (ii),
$$ (3/2) + 1/(2B) -(1/B) \sum_{i=1}^B I_i \geq (3/2) +1/(2B) - 1 \geq 0.$$
Therefore,
$$E =  3/2 + 1/(2B) - \frac{1}{B}\sum_{i=1}^B I_i$$ is a valid $e$-value. Note that $p_B := \frac{1}{B} \sum_{i=1}^B I_i$ is simply the $p$-value corresponding to the right-sided permutation test. For given $\alpha \in (0,1)$, we reject the null hypothesis if and only if $p_B < \alpha$, which is equivalent to rejecting the null hypothesis if and only if 
$$ E > 3/2 + 1/(2B) - \alpha.$$
Therefore, the $p$-permutation test is a special case of the $e$-permutation test. We typically think of $e$-values as controlling FDR, but here $e$-values control type-I error.

We can extend this idea to control family-wise error rate. 

\noindent
\\ \\
\textbf{Example 3}. Suppose we are using CRT or CPT to test conditional independence. If we have misspecified the model for $X|Z$, then the resampled test statistics $T_1, \dots, T_B$ will be off. In particular, qq-plots often reveal that in such settings  the tail of the distribution is incorrect, leading to $p$-value inflation. We can use the proposed framework to downweight re-sampled test statistics in the tail of the distribution, possibly leading to better calibration. For example, consider again example 1, where previously we set $a_i = 1/i$ for all $i$. Suppose that we use (e.g.) a Gaussian kernel to assign the weights. Let
$$\hat{\mu} = \frac{1}{B} \sum_{i=1}^B T_i$$ be the average of the resampled test statistics. Next, let $$K(t) = \frac{1}{\sqrt{2 \pi}} e^{-(1/2)t^2}$$ be the Gaussian kernel. Define $a_i$ as
$$a_i :=(B/i)\frac{K(|T_i - \hat{\mu}|/h) }{\sum_{i=1}^B K(|T_i - \hat{\mu}|/h)  }.$$ Again, set $c = 0$. We see that
$$ \sum_{i=1}^B (a_i) i = \frac{ \sum_{i=1}^B K(|T_i - \hat{\mu}|/h) }{ \sum_{i=1}^B K(|T_i - \hat{\mu}|/h)} = B = B(1 - c),$$ verifying the first condition. Moreover, $A_i \geq 0$ for all $i$, satisfying the second condition. It follows that $$E  := \sum_{i=1}^B a_i I_i$$ is a valid $e$-value. This $e$-value is  less sensitive to the tails of the sampling distribution than the $e$-value from example 1.

\textbf{Advantages to this approach}. The proposed framework has several advantages (I think). First, because the outputs are $e$-values, they can be combined under arbitrary dependence to control FDR using the $e$-BH procedure. Second, the framework is flexible: we can choose among many different ``combining functions'' (including the easy-to-use linear variants) to achieve different objectives. Certain combining functions might have robustness properties, for example. Third, we easily can combine e-values across data splits, enabling us to run multiple train-test splits without worrying about combining the resulting p-values.

\textbf{Open questions}. What is the ``best'' combining function that we could use? 

\textbf{Additional properties of the $I_i$s}. Recall that $I_i = \mathbb{I}(T^* \leq T_i).$ We state several additional properties of the $I_i$s. Let $k_1, k_d, \dots, k_p \in \N$ be indexes such that
$$ 1 \leq k_1 \leq k_2 \leq \dots \leq k_p \leq B.$$ Furthermore, let $r_1, r_2, \dots, r_p \in \N$ be non-negative integers. We have that
$$ I_{k_1}^{r_1} I_{k_2}^{r_2} \dots I_{k_p}^{r_p} = I_{k_1}.$$ The reason is as follows. First, because $I_{k_i}$ is Bernoulli,
$$  I_{k_1}^{r_1} I_{k_2}^{r_2} \dots I_{k_p}^{r_p} = I_{k_1} I_{k_2} \dots, I_{k_p}.$$ Next, we take cases on $T^*$. First, suppose that $T^* \leq k_1$. Then $T^* \leq k_2, \dots, k_p.$ Therefore,
$$I_{k_1} I_{k_2} \dots I_{k_p} = 1 \cdot 1 \dots 1 = I_{k_1}.$$ Next, suppose that $T^* > k_1$. Then $I_{k_1} = 0,$ and so
$$ I_{k_1} I_{k_2} \dots I_{k_p} = 0 \cdot I_{k_2} \dots I_{k_p} = 0 = I_{k_1}.$$
This property is useful because it allows us to more easily evaluate expressions of the form
$\left(\sum_{i=1}^n I_i \right)^r$ for $r \in \N.$ As a warmup, we consider $r = 1$:
$$ \sum_{i=1}^B I_i = a^{(1)}_i I_i,$$ where $a^{(1)}_1 = 1.$ Next, consider $r = 2$:
\begin{multline*}
\left( \sum_{i=1}^B I_i \right)^2 = \sum_{i=1} ^B\sum_{j=1}^B I_i I_j  = \sum_{i=1}^B I_i^2 + 2 \sum_{i = 1}^{B-1} \sum_{j=i+1}^{B} I_i I_j =  \sum_{i=1}^B I_i + 2 \sum_{i=1}^{B-1}  \sum_{j=i+1}^{B} I_i \\ = \sum_{i=1}^B I_i + \sum_{i=1}^{B-1} 2 (B-i) I_i = \sum_{i=1}^B I_i + \sum_{i=1}^B 2(B - i)I_i = \sum_{i=1}^B (2B - 2i + 1)I_i = \sum_{i=1}^B a^{(2)}_i I_i,
\end{multline*} 
where $a^{(2)}_i = 2B - 2i + 1$.
%\begin{multline*}
%\left( \sum_{i=1}^B I_i \right)^3 = \sum_{i=1}^B \sum_{j=1}^B \sum_{k=1}^B I_i I_j I_k = 
% \end{multline*}

Consider the multinomial theorem: 
$$ \left( \sum_{i=1}^B I_i \right)^r = \sum_{k_1 + k_2 + \dots + k_B = r} \binom{ r }{ k_1, k_2, \dots, k_B } \prod_{i=1}^B I_{i}^{k_i},$$ where $$ \binom{r}{k_1, k_2, \dots, k_B} = \frac{r!}{k_1! k_2! \dots k_B!}.$$
Let $$ C(r, B) := \left\{(k_1, k_2, \dots, k_B) \in \{1, \dots, r\}^B : \sum_{i=1}^B k_i = r\right\},$$ i.e., the set of tuples of length $B$ of integers from $1$ to $r$ such that the elements of the  tuple sum to $r$. Let $\tau: C(r,B) \to B$ be defined by
$$\tau(k_1, \dots, k_B) = \min\{i : \{1, \dots, B\} : k_i \geq 1 \},$$ i.e., $\tau$ is the minimal nonzero index of a given configuration. The inverse $\tau^{-1}$ of $\tau$ is the set of configurations with a given minimal nonzero index. That is, for $s \in \{1, \dots, B\}$,
$$ \tau^{-1}(s) = \left\{ (0, \dots, 0, k_s, k_{s+1}, \dots, k_B) : \sum_{i=s}^B k_i = r \right\} .$$
Therefore, the multinomial theorem reduces to
\begin{multline*}
\left( \sum_{i=1}^B I_i \right)^r = \sum_{(k_1, \dots, k_B) : k_1 + \dots + k_B = r} \binom{r}{k_1, k_2, \dots, k_B} I_{\tau(k_1, \dots, k_B)} \\ = \sum_{s=1}^B \sum_{( 0, \dots, 0, k_s, \dots, k_B): k_s \geq 1, k_s + \dots + k_B = r} \binom{r}{k_1, k_2, \dots, k_B} I_s.
\end{multline*}
We consider the above sum:
\begin{multline*}
\sum_{( 0, \dots, 0, k_s, \dots, k_B): k_s \geq 1, k_s + \dots + k_B = r} \binom{r}{k_1, k_2, \dots, k_B} \\ = \sum_{(k_1, \dots, k_{B - s + 1}) : k_1 \geq 1, k_1 + \dots + k_{B - s + 1} = r} \binom{r}{k_1, k_2, \dots, k_B} \\ = \sum_{j=1}^r \sum_{l_1 + \dots + l_{B - s} = r - j} \binom{r}{ j, l_1, \dots, l_{B-s}} \\ = \sum_{j=1}^r \sum_{ l_1 + \dots + l_{B-s} = r-j } \frac{ r! }{ j! l_1! \dots l_{B-s}!} \\ = \sum_{j=1}^r \sum_{l_1 + \dots + l_{B-s} = r-j} \frac{r(r-1)\dots(r-j+1)(r-j)!}{ j! l_1! \dots l_{B-s}!}  \\ = \sum_{j=1}^r \sum_{l_1 + \dots + l_{B-s} = r - j} \frac{r!}{(r-j)!j!} \binom{r-j}{l_1, \dots, l_{B-s}} \\ = \sum_{j=1}^r \frac{r!}{(r-j)!j!} \sum_{l_1, \dots, l_{B-s}} \binom{r-j}{l_1, \dots, l_{B-s}} \\ = \sum_{j=1}^r \binom{r}{j} (B-s)^{r-j}  = \sum_{j=1}^r \binom{r}{j} (B-s)^{r-j} 1^j \\ = \sum_{j=0}^r \binom{r}{j} (B-s)^{r-j}1^j  - 1(B-s)^r = (B - s + 1)^r - (B-s)^r := M(B,r,s).
\end{multline*}
Therefore,
$$\left( \sum_{i=1}^B I_i \right)^r = \sum_{s=1}^B M(B,r,s)I_s.$$ The number $M(B,r,s)$ can be quite large. However, normalizing the sum by $B$ neutralizes this problem:

\begin{multline*}
\left(\frac{1}{B} \sum_{i=1}^B I_i\right)^r = (1/B)^r \sum_{i=1}^B \left[ (B-i+1)^r - (B-i)^r \right]I_i \\ = \sum_{i=1}^B \left[ (1 - i/B + 1/B)^r - (1 - i/B)^r \right] I_i.
\end{multline*}
All terms are easy to compute.

\section{Checking correctness}

Let us check this formula for specific values of $r$. First, setting $r = 1,$ we have $M(B,1,s) = (B - s + 1) - (B - s) = 1$. Therefore, $$\left(\sum_{i=1}^B I_s \right)^1 = \sum_{i=1}^B I_s.$$ Next, setting $r=2$, we have
$$M(B,1,s) = (B+s-1)^2 - (B-s)^2  = 2B - 2i + 1.$$ Therefore,
$$\left( \sum_{i=1}^B I_i \right)^2 = \sum_{i=1}^B [2B - 2i + 1]I_i,$$ which matches what we have above.

\section{Polynomials}
We easily can compute a polynomial of $\sum_{i=1}^B I_i.$ We have that
\begin{multline*}
\left( \sum_{i=1}^B I_i - x_0 \right)^l = \sum_{k=0}^l \binom{l}{k} \left( \sum_{i=1}^B I_i \right)^k x_0^{l - k} \\ = x_0^{l} + \sum_{k=1}^l \binom{l}{k} \left( \sum_{i=1}^B I_i \right)^k x_0^{l-k}   \\ = x^l_0 + \sum_{k=1}^l \binom{l}{k} \left[\sum_{i=1}^B M(B,k,i) I_i  \right]x^{l-k}_0 \\ = x_0^l + \sum_{k = 1}^l \sum_{i=1}^B x_0^{l-k} \binom{l}{k} M(B,k,i) I_i \\ = x_0^l +\sum_{i=1}^B \left[\sum_{k=1}^l x_0^{l-k} \binom{l}{k} M(B,k,i) \right] I_i
\end{multline*}
Finally, for an integer $r$ and polynomial coefficients $a_0, a_1, \dots, a_r \in \R$, we have that
\begin{multline*}
\sum_{l=0}^r a_l \left(S - x_0 \right)^l = a_0 + \sum_{l=1}^r a_l (S - x_0)^l \\ = a_0 + \sum_{l=1}^r a_l \left[x_0^l + \sum_{i=1}^B \left[\sum_{k=1}^l x_0^{l-k} \binom{l}{k} M(B,k,i) \right] I_i \right] \\ = \left(a_0 + \sum_{l=1}^r a_l x_0^l \right) + \sum_{i=1}^B \left[ \sum_{l=1}^r \sum_{k=1}^l a_l x_0^{l-k} \binom{l}{k} M(B,k,i) \right] I_i.
\end{multline*}
Typically, we will have $a_l = f^{(l)}(x_0)/l!,$ where $f$ is a $p$-to-$e$ calibrator. This follows from the following $r$th order Taylor expansion:
$$ f\left(\sum_{i=1}^BI_i \right) \approx \left(a_0 + \sum_{l=1}^r a_l x_0^l \right) + \sum_{i=1}^B \left[ \sum_{l=1}^r \sum_{k=1}^l \frac{f^{(l)}(x_0) x_0^{l-k} \binom{l}{k} M(B,k,i)}{l!} \right] I_i.$$ Of course, the above is a linear combination of the $I_i$s. We define
$$ W(f, x_0, r, B, i) = \sum_{l=1}^r \sum_{k=1}^l \frac{f^{(l)}(x_0) x_0^{l-k} \binom{l}{k} M(B,k,i)}{l!}.$$  Therefore, we can write
$$ f\left(\sum_{i=1}^B I_i \right) \approx \left(a_0 + \sum_{l=1}^r a_l x_0^l \right) + \sum_{i=1}^B W(f, x_0, r, B, i) T_i.$$
In practice, we could compute the $W$ terms ahead of running the algorithm. We even could save these terms in within the package for common choices of $f,$ $x_0$, $r$, and $B$ (e.g., $f(p) = 2p^{-1}$, $x_0 = 1/2$, $r = 20$, $B = 1000$).

\section{$p$-to-$e$ calibrators}
A decreasing function $f : [0,1] \to [0,\infty]$ is a $p$-to-$e$ calibrator if and only if $\int_0^1 f \leq 1.$ It is admissible if and only if $f$ is upper semicontinuous, $f(0) = \infty$, and $\int_0^1 f = 1.$ An example of a $p$-to-$e$ calibrator is
$$f_\kappa(p) = \kappa p^{\kappa - 1}.$$
We derive the Taylor series of $f$. First, we compute the derivatives of $f$. The first derivative is 
$$f_\kappa^{(1)}(p) = \kappa(\kappa - 1) p^{\kappa - 2}.$$
Meanwhile, the second derivative is
$$f^{(2)}_\kappa(p) = \kappa(\kappa - 1) (\kappa - 2) p^{\kappa - 3}.$$ In general, we have that
$$f^{(m)}_\kappa(p) = \prod_{j=0}^m (\kappa - j)p^{\kappa - m- 1}.$$ Therefore, the Taylor series of $f$ at $a$ is
$$ f_\kappa(p) = \sum_{m=0}^\infty \frac{f^{(m)}(a)}{m!} (p - a)^m.$$

\section{Order stat probability}

Let $Y, T_1, \dots, T_B \sim F$. Suppose that $F$ is supported on the real line (the bounded case is similar). Let $X = T_{(B)}$ be the maximum over the $T_i$s. The cumulative density function $F_X$ of $X$ is $F_X(x) = [F(x)]^n$. Further, $F_Y(y)  = F(y).$ We have that
\begin{multline*}
\P(Y \leq X) = \int_{-\infty}^{\infty} \int_{y}^\infty f(y)f(x) dx dy = \int_{-\infty}^\infty f(y) \left[ \int_{y}^\infty f(x) dx \right] dy \\ = \int_{-\infty}^\infty f(y) \left[ F_X(\infty) - F_X(y) \right] dy = \int_{-\infty}^\infty f(y) [1 - (F(y))^n] dy \\= \int_{-\infty}^\infty f(y) - f(y)F(y)^n dy = F(y) - \frac{1}{n+1}[F(y)]^{n+1} \big|^\infty_{-\infty} = 1 - \frac{1}{n+1} = \frac{n}{n+1}.
\end{multline*}

Now, in the case that $r = n$, we have that $F_X(x) = [F(x)]^n$, and the problem is easy (see above). We now consider the more general case. We have the basic identity
$$ \int_{-\infty}^\infty f(y) F^m(y) dy = \frac{1}{m+1}.$$
We want to show that $$\P(Y \leq X_r) = 1 - \frac{n + 1 - r}{n+1} = \frac{r}{n+1}.$$ For a given function $F_{X_r}(y),$ let $L(F_{X_r}(y))$ denote the linear operation
$$ L(F_{X_r}(y)) = \int_{-\infty}^\infty f(y) F_{X_r}(y) dy.$$ We equivalently want to show that
$$ L(F_{X_r}(y)) = \frac{n+1-r}{n+1},$$
where
$$F_{X_r}(y) = \sum_{j=r}^n \binom{n}{j} [F(y)]^j[1 - F(y)]^{n-j}$$

For example, when $r=n$, $F_{X_n}(y) = [F(y)]^n,$ so $L(F_{X_n}(y)) = \frac{1}{n+1}.$ Next, when $r = n - 1$,
$$F_{X_{n-1}}(y) = \binom{n}{n-1} [F(y)]^{n-1} [1 - F(y)] + F_{X_n}(y).$$
Considering the first piece, we have that
\begin{multline*}
L( n[F(y)]^{n-1}[1-F(y)]) = nL( F^{n-1}(y) - F^n(y)) = n \left( \frac{1}{n} - \frac{1}{n+1} \right) \\ = 1 - \frac{n}{n+1} = \frac{1}{n+1}.
\end{multline*}
Combining both pieces,
$$ L(F_{X_{n-1}}(y)) = \frac{1}{n+1} + \frac{1}{n+1} = \frac{2}{n+1} .$$
Next, we examine
$$F_{X_{n-2}}(y) = \binom{n}{n-2} [F(y)]^{n-2} [1 - F(y)]^{2} + F_{X-1}(y).$$
Focusing on the first piece, we have that
\begin{multline*}
F^{n-2}(y)[1 - F(y)]^2 = F^{n-2}(y) (1 - 2F(y) + F^2(y)) \\ = F^{n-2}(y) - 2 F^{n-1}(y) + F^n(y).
\end{multline*}
Applying the linear operator and multiplying by the binomial coefficient,
\begin{multline*}
\binom{n}{n-1} \left[ L( F^{n-2}(y) - 2 F^{n-1}(y) + F^n(y))\right] = \binom{n}{n-1}\left[ \frac{1}{n-1} -\frac{2}{n} + \frac{1}{n+1} \right] \\ = \frac{1}{n+1}.
\end{multline*}
Therefore, $L(F_{X-2}(y)) = 3/(n+1),$ as desired. Next, let $r \in \{1, \dots, n\}$ be arbitrary. We want to show that
$$\binom{n}{r} L\left( F^r(y) [1 - F(y)]^{n-r}\right) = \frac{1}{n+1}.$$ 
The binomial theorem implies
$$[1- F(y)]^{n-r} = \sum_{k=0}^{n-r} \binom{n-r}{k} F^{k}(y) (-1)^k,$$ and so
$$F^{r}(y) [1 - F(y)]^{n-r} = \sum_{k=0}^{n-r} \binom{n-r}{k} F^{k+r}(y) (-1)^k,$$ implying
$$ \binom{n}{r}L( F^{r}(y) [1 - F(y)]^{n-r} ) = \binom{n}{r} \sum_{k=0}^{n-r} \binom{n-r}{k} \frac{(-1)^k}{k+r+1} := Q(n,r).$$
Surry 2004 Corollary 2.2 implies
$$ \sum_{k=1}^{n-r} \binom{n-r}{k} \frac{(-1)^k}{k+r+1} = \frac{1}{ (n+1) \binom{n}{r}}.$$ Therefore,
$$\binom{n}{r} L(F^r(y)[1 - F(y)]^{n-r}) = \frac{1}{n+1}.$$
We conclude that 
$$ L(F_{X_r}(y)) = \frac{n+1-r}{n+1},$$
implying $$\P(Y \leq X_r) = \frac{r}{n+1},$$ as desired.

Suppose without loss of generality that $F$ is supported on the real line (the proof is similar in the case of bounded $F$).  For notational simplicity, let $Y = T^*$. Denote the density of $F$ by $f$ (i.e., $f = F'$). Denote the density of $Y$, $T_i$, and $(Y,T_i)$ by $f_Y = f$, $f_{T_i}$, and $f_{Y, T_i}$ respectively. Standard distributional theory on order statistics indicates that the CDF $F_{T_i}$ of $T_i$ is given by $$ F_{T_i}(t) = \sum_{j=i}^B \binom{B}{j}[F(t)]^j [1 - F(t)]^{B-j}.$$ Meanwhile, the CDF $F_Y$ of $Y$ is given by $F_Y = F$. We seek to show that $\P(Y \leq T_i) = i/(B+1).$ We have that
\begin{multline}\label{pf_6}
\P(Y \leq T_i) = \int_{-\infty}^{\infty} \int_{y}^\infty f_{Y,T_i}(y,t) dt dy = \int_{-\infty}^{\infty} \int_{y}^\infty f_Y(y)f_{T_i}(t) dt dy \\ = \int_{-\infty}^\infty f(y) \left[\int_{y}^\infty f_{T_i}(t) dt \right] dy = \int_{-\infty}^\infty f(y) \left[ F_{T_i}(\infty) - F_{T_i}(y) \right] dy = 1 - \int_{-\infty}^\infty f(y) F_{T_i}(y)dy. 
\end{multline}
The problem reduces to evaluating the integral in the final equality of (\ref{pf_6}). We have that
\begin{multline}\label{pf_7}
\int_{-\infty}^\infty f(y) T_{T_i}(y) dy = \int_{-\infty}^\infty f(y) \sum_{j=i}^B \binom{B}{j}F^j(y) [1 - F(y)]^{B-j}dt \\ = \int_{-\infty}^\infty f(y) \sum_{j=i}^B \binom{B}{j} F^j(y) \sum_{k=0}^{B-j}\binom{B-j}{k} (-1)^k F^k(y) dt\\ = \sum_{j=i}^B \binom{B}{j} \sum_{k=0}^{B-j} \binom{B-j}{k}(-1)^k\int_{-\infty}^\infty f(y) F^{j+k}(y) dy \\ = \sum_{j=i}^B \binom{B}{j} \sum_{k=0}^{B-j} \binom{B-j}{k} (-1)^k \left[ \frac{1}{1+j+k} F^{j+k+1}(y)|^\infty_{-\infty} \right] \\ = \sum_{j=i}^B \binom{B}{j} \sum_{k=0}^{B-j} \binom{B-j}{k} (-1)^k \left[ \frac{1}{1+j+k}\right].
\end{multline}
\cite{Sury2004} (Corollary 2.2) showed that
$$ \sum_{k=0}^{B-j} (-1)^k \binom{B-j}{k} \frac{1}{1 + j + k} = \frac{1}{(B + 1) \binom{ B  }{j}}.$$
Substituting the above into (\ref{pf_7}), we obtain.
\begin{multline}\label{pf_8}
\sum_{j=i}^B \binom{B}{j} \sum_{k=0}^{B-j} \binom{B-j}{k} (-1)^k \left[ \frac{1}{1+j+k}\right] =  \\ \sum_{j=i}^B \binom{B}{j} \frac{1}{(B+1)\binom{B}{j}} = \sum_{j=i}^B \frac{1}{B+1} = \frac{B-i+1}{B+1}.
\end{multline}
Finally, combining (\ref{pf_6}), (\ref{pf_7}), and (\ref{pf_8}), we conclude that
$$\P(Y \leq T_i) = \P(T^* \leq T_i) = \frac{B+1}{B+1} - \frac{B-i+1}{B+1} = \frac{i}{B+1}.$$


% We also can write
%$$Q(n,r) = \sum_{k=0}^{n-r} \binom{n}{r+k} \binom{r+k}{r} \frac{(-1)^k}{k+r+1}.$$
%For fixed $r$ and $n+1$, we have
%\begin{multline*}
%Q(n,r) = \sum_{k=0}^{n-r} \binom{n}{r+k} \binom{r+k}{r} \frac{(-1)^k}{k+r+1} \\ = \sum_{k=0}^{n+1-r} \binom{n+1}{r+k} \binom{r+k}{r} \frac{(-1)^k}{k+r+1} \\ = \sum_{k=0}^{n+1-r} \left[ \binom{n}{r+k} + \binom{n}{r+k-1} \right] \binom{r+k}{r} \frac{(-1)^k}{k+r+1} \\ = \sum_{k=0}^{n+1-r} \binom{n}{r+k} \binom{r+k}{r} \frac{(-1)^k}{k+r+1} + \sum_{k=0}^{n+1-r}  \binom{n}{r+k-1} \binom{r+k}{r} \frac{(-1)^k}{k+r+1}
% \end{multline*}
%\\ \noindent
%\textbf{Base case}: Let $n = r$. Then $$Q(n,n) = \binom{ n }{ n } \sum_{k=0}^{n-n} \binom{n-n}{k} \frac{(-1)^k}{k + n + 1} = \frac{1}{0 + n + 1} = \frac{1}{n+1}.$$
%\textbf{Assumption}: For some $n > r,$ assume that the formula holds, i.e.
%$$Q(n,r) = \binom{n}{r} \sum_{k=0}^{n-r} \binom{n-r}{k} \frac{(-1)^k}{k + r + 1} = \frac{1}{n+1}.$$
%\textbf{Induction step}: We consider the case of $n+1;$ we seek to show that $Q(n+1,r) = 1/(n+2).$ Now,
%\begin{multline*}
%Q(n+1, r) = \binom{n+1}{r} \sum_{k=0}^{n+1-r} \binom{n+1-r}{k} \frac{(-1)^k}{k + r + 1} \\ = \left[ \binom{n}{r} + \binom{n}{r-1} \right] \sum_{k=0}^{n+1-r} \left[\binom{n - r}{k} + \binom{n-r}{k-1} \right ]  \frac{(-1)^k}{k + r + 1}
%\end{multline*}


%First, for given $c_i, a_i \in \R$ and $i \in \Z^{\geq 0}$, we have that
%$$t'(x) = \frac{d}{dx} a_i (x-c_i) \log^i(x) = a_i \log^i(x) + (1/x)(a_i)(i)(x - c_i)\log^{i-1}(x) .$$ Next, we have that
%$$
%b'(x) = \frac{d}{dx} x^{r+1} \log^{r+2}(x) = x^{r}(r+1) \log^{r+2}(x) + x^{r} (r + 2)\log^{r+1}(x).
%$$
%Thus, suppose that we have constants $a_0, \dots, a_r$ and $c_0, \dots, c_r$ such that
%$$h(x) = \frac{\sum_{i=0}^r a_i(x - c_i) \log^i(x)}{x^{r+1} \log^{r+2}(x)} = \frac{t(x)}{b(x)}.$$
%Now, we evaluate $t'$. We have that
%\begin{multline*}
%t'(x) = \sum_{i=0}^r a_i \log^i(x) + (1/x)(a_i)(i)(x - c_i)\log^{i-1}(x) \\ = a_r \log^{r}(x) + \sum_{i=1}^{r-1} \log^{i}(x) \left[ a_{i-1} + (1/x)(a_i)(i)(x-c_i) \right].
%\end{multline*}
%We additionally have that
%$$t(x) = a_0(x-c_0) + \left[\sum_{i=1}^{r-1} a_i(x-c_i)\log^i(x)\right] + (a_r)(x-c_r)\log^r(x).$$
%Hence,
%$$
%t'(x) b(x)  = b(x) a_r \log^r(x) + b(x) \sum_{i=1}^{r-1} \log^{i}(x) \left[ a_{i-1} + (1/x)(a_i)(i)(x-c_i) \right]. 
%$$
%Next,
%\begin{multline*}
%t'(x) = \sum_{i=0}^r a_i \log^i(x) + (1/x)(a_i)(i)(x - c_i)\log^{i-1}(x) = \\ a_0 + \left[ a_1 \log^1(x) + (1/x) (a_1)(1)(x-c_1) \log^0(x) \right] \\ + [ a_2 \log^2(x) + (1/x)(a_2)(2)(x-c_2) \log^1(x) ] \\ + [a_3\log^3(x) + (1/x)(a_3)(3)(x-c_3)\log^2(x)] \dots \\ + [a_{r-1} \log^{r-1}(x) + (1/x) a_{r-1}(r-1)\log^{r-2}(x)] \\ + [ a_r \log^r(x) + (1/x)(a_r)(r)(x - c_r) \log^{r-1}(x)] \\ \\ = [a_0\log^0(x) + (1/x)(a_1)(1)(x-c_1)\log^0(x)] \\ + [ a_1 \log^1(x) + (1/x)(a_2)(2)(x-c_2)\log^1(x)] \\ + [a_2 \log^2(x) + (1/x) (a_3)(3)(x-c_3)\log^2(x)] + \dots \\ + [a_{r-1} \log^{r-1}(x) + (1/x)(a_r)(r)(x-c_r)\log^{r-1}(x)] + a_r\log^r(x) \\ = 
%a_r \log^{r}(x) + \sum_{i=0}^{r-1} \log^{i}(x) \left[ a_{i-1} + (1/x) a_i(i)(x-c_i) \right].
%\end{multline*}

\end{document}
