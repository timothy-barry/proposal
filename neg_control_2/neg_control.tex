\documentclass[12pt]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx} 
\usepackage{float}
\usepackage[caption = false]{subfig}
\usepackage[
backend=biber,
style=authoryear,
maxcitenames=3,
maxbibnames=3,
uniquelist=false,
backref=true,
doi=false,
isbn=false,
url=false,
eprint=false,
backref=false
]{biblatex}
\addbibresource{/Users/timbarry/optionFiles/tims_proposal.bib}

\begin{document}
	\noindent
	Tim B.
	\begin{center}
		\textbf{The (overlooked?) importance of negative controls in conditional independence testing}
	\end{center}

Conditional independence (CI) tests asses the association between two variables (e.g., a genetic variant and a phenotype) while controlling for a vector of confounders (e.g., population structure). CI tests are among the most fundamental and widely-used hypothesis tests in the sciences, technology, and other areas. Despite their importance, CI tests pose a basic and unavoidable difficulty: assumption-free CI testing is impossible \parencite{Shah2020,Kim2021}. Put differently, all valid CI tests must make an assumption (or set of assumptions) about the data-generating process. In practice these assumptions often go unchecked, greatly limiting the trustworthiness of results. This is not due to negligence; to the contrary, checking the assumptions of CI tests is fraught with difficulties (and in some settings is impossible), as we will show. We therefore face a dilemma as data analysts: we are obligated to check the assumptions of CI testing procedures to ensure reliableness of the results, but very often this task is challenging (or even impossible) to carry out in practice.

Our core thesis is that ``negative controls'' -- external samples for which the null hypothesis is known to be true, roughly  -- are crucially important (and in some cases \textit{required}) for verifying the assumptions of CI testing procedures, enabling rigorous inference. We work in the modern ``high-multiplicity'' setting in which we seek to test thousands (or more) of hypotheses and produce a discovery set with guaranteed false discovery rate (FDR) control \parencite{Benjamini1995, Li2021}.

This writeup is organized as follows. First, we briefly summarize the vast and growing landscape of CI testing procedures, omitting from our discussion those procedures that do not enable the selection of critical regions (and thus the control of FDR). We argue that negative controls play (or ought to play) a crucial role in high-multiplicity CI testing. We describe two broad types of negative controls --- ``experimental'' negative controlsÂ and ``in silico'' negative controls --- and argue that, although the former are superior statistically, the latter can be constructed directly from the data in many applications and are therefore more broadly available.

Next, we introduce several new strategies for working effectively with negative control data in high-multiplicity hypothesis testing problems. (The discussion here extends beyond CI testing.) We propose to calibrate the testing procedure against \textit{both} the empirical negative control distribution \textit{and} the theoretical null distribution, satisfying an appealing double-robustness property. Furthermore, we introduce a simple and practical method for assessing whether a given procedure is robust to inflation in the tail of the empirical null distribution. To provide a visual diagnostic to aid in the application of the above method, we introduce the ``symmetry plot'' (or ``s-plot''), a nonparametric analogue of the commonly-used quantile-quantile plot (qq-plot). % Throughout, we lean heavily on the Barber-Candes (BC) multiple testing procedure \parencite{Barber2015}, as we find this approach to be flexible, extensible, and powerful. 
We illustrate these ideas through extensive analyses of a new kind of genomic data that combines CRISPR genome editing with single-cell RNA sequencing.

As an auxiliary contribution, we introduce a new class of fast and powerful Gaussian test statistics for use in a broad range of existing CI testing methods, including the conditional randomization test, the conditional permutation test, and the local permutation test \parencite{Candes2018a,Berrett2020,Kim2021}. The idea behind these statistics is to repeatedly fit OLS, ridge regression, or additive spline models to the permuted (or resampled) data via an online QR decomposition algorithm, achieving high power and speed.

\printbibliography
 
\end{document}